---
layout: single
title: "Minority-Oriented Vicinity Expansion with Attentive Aggregation for Video Long-Tailed Recognition (AAAI23)"
categories: AAAI2023
toc: true
toc_sticky: true
toc_label: Index
---

  **WonJun Moon** &emsp; **Hyun Seok Seong** &emsp; **Jae-Pil Heo** 
{: .text-center}
 **Sungkyunkwan University** 
{: .text-center}

<p></p>

![](/assets/images/MOVE/overview.png)

## Abstract
A dramatic increase in real-world video volume with extremely diverse and emerging topics naturally forms a long-tailed video distribution in terms of their categories, and it spotlights the need for Video Long-Tailed Recognition (VLTR). In this work, we summarize the challenges in VLTR and explore how to overcome them. The challenges are: (1) it is impractical to re-train the whole model for high-quality features, (2) acquiring frame-wise labels requires extensive cost, and (3) long-tailed data triggers biased training. Yet, most existing works for VLTR unavoidably utilize image-level features extracted from pretrained models which are task-irrelevant, and learn by video-level labels. Therefore, to deal with such (1) task-irrelevant features and (2) video-level labels, we introduce two complementary learnable feature aggregators. Learnable layers in each aggregator are to produce task-relevant representations, and each aggregator is to assemble the snippet-wise knowledge into a video representative. Then, we propose Minority-Oriented Vicinity Expansion (MOVE) that explicitly leverages the class frequency into approximating the vicinity distributions to alleviate (3) biased training. By combining these solutions, our approach achieves state-of-the-art results on large-scale VideoLT and synthetically induced Imbalanced-MiniKinetics200. With VideoLT features from ResNet-50, it attains 18% and 58% relative improvements on head and tail classes over the previous state-of-the-art method, respectively.


## Method

### Aggregators

- Weak supervision problem (one class label per multiple-frame video) cannot be handled as noisy-label problem because of biased training in long-tailed circumstances.
- Pretrained features need to be finetuned for the downstream task.
- Thus, we use aggregators to aggregate frames into a video prototype. (2 aggregators to minimize the information loss while aggregating the frame-level features.)

![](/assets/images/MOVE/attention.png){:.align-center}

### Minority Oriented Vicinity Expansion
- Long-tailed distribution causes biased training.
- To expand and extend the boundaries of the minority classes, we use dynamic extrapolation and calibrated interpolation.
- Dynamic extrapolation is implemented between two prototypes of the same instance to prevent generating outliers.
- And to only diversify the only tail classes, we propose dynamic frame sampler.
- After, calibrated interpolation is implemented to extend the boundaries of the tail classes.

![](/assets/images/MOVE/dfs.png){:.img-width-52.align-left}
![](/assets/images/MOVE/method2.png){:.img-width-43.align-right}

<br/><br/><br/><br/><br/><br/><br/><br/>

[//]: # (![]&#40;/assets/images/MOVE/dfs.png&#41;{:.img-width-half.align-center})

[//]: # (![]&#40;/assets/images/MOVE/method2.png&#41;{:.img-width-half.align-center})

## Experiment 
- In addition to VideoLT, the large scale video long-tailed dataset, we also introduce and run experiments on Imbalanced-MiniKinetics200.
![](/assets/images/MOVE/exp1.png){:.align-center}
![](/assets/images/MOVE/exp2.png){:.align-center}

## Dataset (Imbalanced-MiniKinetics200)
- Imbalanced-MiniKinetics200 consists of 200 classes which are extracted from Kinetics-400.
- Features and original files are available through the google drive link.
<a href="https://drive.google.com/drive/folders/1ZaXo0a7eEStSaIfv1ql6Qex8AeVl2VOX?usp=sharing" target="_blank" rel="noopener noreferrer" class="btn btn--info"> <span style="color:blue"> Imb.MiniKinetics200 Dataset </span></a>

## Bibtex 
```
@inproceedings{moon2023minority,
  title={Minority-Oriented Vicinity Expansion with Attentive Aggregation for Video Long-Tailed Recognition},
  author={Moon, WonJun and Seong, Hyun Seok and Heo, Jae-Pil},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  number={2},
  pages={1931--1939},
  year={2023}
}
```


## Link 
<td> 
<a href="https://arxiv.org/abs/2211.13471" target="_blank" rel="noopener noreferrer" class="btn btn--info"> <span style="color:blue"> Arxiv </span></a>
<a href="https://ojs.aaai.org/index.php/AAAI/article/view/25284/25056" target="_blank" rel="noopener noreferrer" class="btn btn--info"> <span style="color:blue"> Paper </span></a>
<a href="https://github.com/wjun0830/MOVE" target="_blank" rel="noopener noreferrer" class="btn btn--info"> <span style="color:blue"> Code </span></a>
<a href="https://www.youtube.com/watch?v=SbXsj_pc_-c&t=301s" target="_blank" rel="noopener noreferrer" class="btn btn--info"> <span style="color:blue"> Video </span></a>
</td>
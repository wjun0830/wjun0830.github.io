---
layout: home
title: "About"
order: 1
# toc: true
# toc_sticky: true
# toc_label: Index

---

Hello. I'm WonJun Moon, a Ph.D. candidate at Sungkyunkwan university, South Korea.
I am a member of <a href="https://sites.google.com/site/vclabskku/intro?authuser=0" target="_blank" rel="noopener noreferrer"><span style="color:#6AADC2">Visual Computing LABoratory</span></a> advised by Prof. <a href="https://scholar.google.com/citations?hl=ko&user=VXyJ_ssAAAAJ" target="_blank" rel="noopener noreferrer"><span style="color:#6AADC2">Jae-Pil Heo</span></a>.
My current research interests primarily lie in Video Understanding and Multi-modal Learning.
Most recently, I am interested in Video Object-Centric Learning to establish efficient video representations.
Besides, my other interests include Video Retrieval, Representation Learning, Segmentation, and Few-Shot Learning.


<!-- ## News -->
<!-- * (2024. 01) 2023 President's List, Sungkyunkwan University.
* (2024. 07) 2 papers accepted to ECCV 2024.
* (2023. 12) 1 paper accepted to AAAI 2024.
* (2023. 05) Selected as an 18th scholarship student of the Gwanjeong Educational Foundation.
* (2023. 03) 2 papers accepted to CVPR 2023.
<!-- * (2022. 11) 1 paper accepted to AAAI 2023.
* (2022. 07) 2 papers accepted to ECCV 2022.
* (2022. 03) 1 paper accepted to CVPR 2022.
* (2021) AI Challenge hosted by Ministry of Science and ICT, South Korea : Selected/Funded as a good research team.  -->


## Publications
<p style="font-size:15px"> (&#42; &#58; equal contribution) </p>

### International
<table class="table table-sm table-borderless">

<tr>
<th scope="row"> ICCV 2025 </th>
<td> From Vicious to Virtuous Cycles: Synergistic Representation Learning for Unsupervised Video Object-Centric Learning
<br> 
<!-- <a href="" target="_blank" rel="noopener noreferrer"> <span style="color:blue"> [Arxiv] </span></a> -->
<a href="https://openreview.net/forum?id=bWoT6Z21rH&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3DICLR.cc%2F2026%2FConference%2FAuthors%23your-submissions)" target="_blank" rel="noopener noreferrer"> <span style="color:blue"> [Paper] </span> </a>
<!-- <a href="" target="_blank" rel="noopener noreferrer"> <span style="color:blue"> [Code] </span></a> -->
</td>
<td> Hyun Seok Seong<sup>&#42;</sup>, <u>WonJun Moon<sup>&#42;</sup></u>, Jae-Pil Heo </td>
</tr>

<tr>
<th scope="row"> NeurIPS 2025 </th>
<td> Mitigating Semantic Collapse in Partially Relevant Video Retrieval
<br> 
<a href="" target="_blank" rel="noopener noreferrer"> <span style="color:blue"> [Arxiv] </span></a>
<!-- <a href="" target="_blank" rel="noopener noreferrer"> <span style="color:blue"> [Paper] </span> </a> -->
<!-- <a href="" target="_blank" rel="noopener noreferrer"> <span style="color:blue"> [Code] </span></a> -->
</td>
<td> <u>WonJun Moon<sup>&#42;</sup></u>, MinSeok Jung<sup>&#42;</sup>, Gilhan Park, Tae-Young Kim, Cheol-Ho Cho, Woojin Jun, Jae-Pil Heo </td>
</tr>

<tr>
<th scope="row"> ICCV 2025 </th>
<td> Selective Contrastive Learning for Weakly Supervised Affordance Grounding
<br> 
<a href="https://arxiv.org/abs/2508.07877" target="_blank" rel="noopener noreferrer"> <span style="color:blue"> [Arxiv] </span></a>
<!-- <a href="" target="_blank" rel="noopener noreferrer"> <span style="color:blue"> [Paper] </span> </a> -->
<a href="https://github.com/hynnsk/SelectiveCL" target="_blank" rel="noopener noreferrer"> <span style="color:blue"> [Code] </span></a>
</td>
<td> <u>WonJun Moon<sup>&#42;</sup></u>, Hyun Seok Seong<sup>&#42;</sup>, Jae-Pil Heo </td>
</tr>

<tr>
<th scope="row"> ICCV 2025 </th>
<td> Prototypes are Balanced Units for Efficient and Effective Partially Relevant Video Retrieval
<br> 
<a href="https://arxiv.org/abs/2504.13035" target="_blank" rel="noopener noreferrer"> <span style="color:blue"> [Arxiv] </span></a>
<!-- <a href="" target="_blank" rel="noopener noreferrer"> <span style="color:blue"> [Paper] </span> </a> -->
<!-- <a href="" target="_blank" rel="noopener noreferrer"> <span style="color:blue"> [Code] </span></a> -->
</td>
<td> <u>WonJun Moon</u>, Cheol-Ho Cho, Woojin Jun, Minho Shim, Taeoh Kim, Inwoong Lee, Dongyoon Wee, Jae-Pil Heo </td>
</tr>

<tr>
<th scope="row"> CVPR 2025 (<b>oral</b>)</th>
<td> Temporal Alignment-Free Video Matching for Few-shot Action Recognition
<br> 
<a href="https://arxiv.org/abs/2504.05956" target="_blank" rel="noopener noreferrer"> <span style="color:blue"> [Arxiv] </span></a>
<a href="https://openaccess.thecvf.com/content/CVPR2025/html/Lee_Temporal_Alignment-Free_Video_Matching_for_Few-shot_Action_Recognition_CVPR_2025_paper.html" target="_blank" rel="noopener noreferrer"> <span style="color:blue"> [Paper] </span> </a>
<a href="https://github.com/leesb7426/TEAM" target="_blank" rel="noopener noreferrer"> <span style="color:blue"> [Code] </span></a>
</td>
<td> SuBeen Lee, <u>WonJun Moon</u>, Hyun Seok Seong, Jae-Pil Heo </td>
</tr>

<tr>
<th scope="row"> AAAI 2025 </th>
<td> Ambiguity-Restrained Text-Video Representation Learning for Partially Relevant Video Retrieval
<br> 
<a href="https://www.arxiv.org/abs/2506.07471" target="_blank" rel="noopener noreferrer"> <span style="color:blue"> [Arxiv] </span></a>
<!-- <a href="https://ojs.aaai.org/index.php/AAAI/article/view/28050" target="_blank" rel="noopener noreferrer"> <span style="color:blue"> [Paper] </span> </a> -->
<!-- <a href="https://github.com/Seunggu0305/VLCounter" target="_blank" rel="noopener noreferrer"> <span style="color:blue"> [Code] </span></a> -->
</td>
<td> Cheol-Ho Cho, <u>WonJun Moon</u>, Woojin Jun, MinSeok Jung, Jae-Pil Heo</td>
</tr>

<tr>
<th scope="row"> AAAI 2025 </th>
<td> Bridging the Semantic Granularity Gap Between Text and Frame Representations for Partially Relevant Video Retrieval
<br> 
<!-- <a href="https://arxiv.org/abs/2312.16580" target="_blank" rel="noopener noreferrer"> <span style="color:blue"> [Arxiv] </span></a> -->
<a href="https://ojs.aaai.org/index.php/AAAI/article/view/32437" target="_blank" rel="noopener noreferrer"> <span style="color:blue"> [Paper] </span> </a>
<!-- <a href="https://github.com/Seunggu0305/VLCounter" target="_blank" rel="noopener noreferrer"> <span style="color:blue"> [Code] </span></a> -->
</td>
<td> Woojin Jun, <u>WonJun Moon</u>, Cheol-Ho Cho, MinSeok Jung, Jae-Pil Heo</td>
</tr>

<tr>
<th scope="row"> TPAMI 2024 </th>
<td> Task-oriented channel attention for fine-grained few-shot classification
<br> 
<a href="https://arxiv.org/abs/2308.00093" target="_blank" rel="noopener noreferrer"> <span style="color:blue"> [Arxiv] </span></a>
<a href="https://ieeexplore.ieee.org/abstract/document/10763467" target="_blank" rel="noopener noreferrer"> <span style="color:blue"> [Paper] </span> </a>
<!-- <a href="https://github.com/leesb7426/TEAM" target="_blank" rel="noopener noreferrer"> <span style="color:blue"> [Code] </span></a> -->
</td>
<td> SuBeen Lee, <u>WonJun Moon</u>, Hyun Seok Seong, Jae-Pil Heo </td>
</tr>


<tr>
<th scope="row"> ECCV 2024 </th>
<td> Progressive Proxy Anchor Propagation for Unsupervised Semantic Segmentation
<br> 
<a href="https://arxiv.org/abs/2407.12463" target="_blank" rel="noopener noreferrer"> <span style="color:blue"> [Arxiv] </span></a>
<!-- <a href="" target="_blank" rel="noopener noreferrer"> <span style="color:blue"> [Paper] </span> </a> -->
<a href="https://github.com/hynnsk/PPAP" target="_blank" rel="noopener noreferrer"> <span style="color:blue"> [Code] </span></a>
</td>
<td> Hyun Seok Seong, <u>WonJun Moon</u>, SuBeen Lee, Jae-Pil Heo </td>
</tr>

<tr>
<th scope="row"> ECCV 2024 </th>
<td> Mitigating Background Shift in Class-Incremental Semantic Segmentation
<br> 
<a href="https://arxiv.org/abs/2407.11859" target="_blank" rel="noopener noreferrer"> <span style="color:blue"> [Arxiv] </span></a>
<!-- <a href="" target="_blank" rel="noopener noreferrer"> <span style="color:blue"> [Paper] </span> </a> -->
<a href="https://github.com/RoadoneP/ECCV2024_MBS" target="_blank" rel="noopener noreferrer"> <span style="color:blue"> [Code] </span></a>
</td>
<td> Gilhan Park, <u>WonJun Moon</u>, SuBeen Lee, Tae-Young Kim, Jae-Pil Heo </td>
</tr>

<tr>
<th scope="row"> arXiv 2024 </th>
<td> Correlation-Guided Query-Dependency Calibration for Video Temporal Grounding
<br> 
<a href="https://arxiv.org/abs/2311.08835" target="_blank" rel="noopener noreferrer"> <span style="color:blue"> [Arxiv] </span></a>
<!-- <a href=" " target="_blank" rel="noopener noreferrer"> <span style="color:blue"> [Paper] </span></a> -->
<a href="https://github.com/wjun0830/CGDETR" target="_blank" rel="noopener noreferrer"> <span style="color:blue"> [Code] </span></a> 
<!-- <a href="https://www.youtube.com/watch?v=df-gtJcZEw8&t=301s" target="_blank" rel="noopener noreferrer"> <span style="color:blue"> [Video] </span></a>  -->
</td>
<td> <u>WonJun Moon</u>, Sangeek Hyun, SuBeen Lee, Jae-Pil Heo </td> 
</tr>

<tr>
<th scope="row"> AAAI 2024 </th>
<td> VLCounter: Text-aware Visual Representation for Zero-Shot Object Counting
<br> 
<a href="https://arxiv.org/abs/2312.16580" target="_blank" rel="noopener noreferrer"> <span style="color:blue"> [Arxiv] </span></a>
<a href="https://ojs.aaai.org/index.php/AAAI/article/view/28050" target="_blank" rel="noopener noreferrer"> <span style="color:blue"> [Paper] </span> </a>
<a href="https://github.com/Seunggu0305/VLCounter" target="_blank" rel="noopener noreferrer"> <span style="color:blue"> [Code] </span></a>
</td>
<td> Seunggu Kang, <u>WonJun Moon</u>, Euiyeon Kim, Jae-Pil Heo </td>
</tr>

<tr>
<th scope="row"> CVPR 2023 </th>
<td> Query-Dependent Video Representation for Moment Retrieval and Highlight Detection
<br> 
<a href="https://arxiv.org/abs/2303.13874" target="_blank" rel="noopener noreferrer"> <span style="color:blue"> [Arxiv] </span></a>
<a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Moon_Query-Dependent_Video_Representation_for_Moment_Retrieval_and_Highlight_Detection_CVPR_2023_paper.pdf" target="_blank" rel="noopener noreferrer"> <span style="color:blue"> [Paper] </span></a>
<a href="https://github.com/wjun0830/QD-DETR" target="_blank" rel="noopener noreferrer"> <span style="color:blue"> [Code] </span></a> 
<a href="https://www.youtube.com/watch?v=df-gtJcZEw8&t=301s" target="_blank" rel="noopener noreferrer"> <span style="color:blue"> [Video] </span></a> 
</td>
<td> <u>WonJun Moon<sup>&#42;</sup></u>, Sangeek Hyun<sup>&#42;</sup>, Sanguk Park, Dongchan Park, Jae-Pil Heo </td> 
</tr>


<tr>
<th scope="row"> CVPR 2023 </th>
<td> Leveraging Hidden Positives for Unsupervised Semantic Segmentation
<br> 
<a href="https://arxiv.org/abs/2303.15014" target="_blank" rel="noopener noreferrer"> <span style="color:blue"> [Arxiv] </span></a>
<a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Seong_Leveraging_Hidden_Positives_for_Unsupervised_Semantic_Segmentation_CVPR_2023_paper.pdf" target="_blank" rel="noopener noreferrer"> <span style="color:blue"> [Paper] </span> </a>
<a href="https://github.com/hynnsk/HP" target="_blank" rel="noopener noreferrer"> <span style="color:blue"> [Code] </span></a>
</td>
<td> Hyun Seok Seong, <u>WonJun Moon</u>, SuBeen Lee, Jae-Pil Heo </td>
</tr>


<tr>
<th scope="row"> AAAI 2023 (<b>oral</b>)</th>

<td> Minority-Oriented Vicinity Expansion with Attentive Aggregation for Video Long-Tailed Recognition 
<br> 
<a href="https://arxiv.org/abs/2211.13471" target="_blank" rel="noopener noreferrer"> <span style="color:blue"> [Arxiv] </span></a>
<a href="https://ojs.aaai.org/index.php/AAAI/article/view/25284/25056" target="_blank" rel="noopener noreferrer"> <span style="color:blue"> [Paper] </span></a>
<a href="https://github.com/wjun0830/MOVE" target="_blank" rel="noopener noreferrer"> <span style="color:blue"> [Code] </span></a>
<a href="https://www.youtube.com/watch?v=SbXsj_pc_-c&t=301s" target="_blank" rel="noopener noreferrer"> <span style="color:blue"> [Video] </span></a> 
</td>
<td> <u>WonJun Moon</u>, Hyun Seok Seong, Jae-Pil Heo </td>
</tr>

<tr>
<th scope="row"> ECCV 2022 </th>
<td> Tailoring Self-Supervision for Supervised Learning 
<br> 
<a href="https://arxiv.org/abs/2207.10023" target="_blank" rel="noopener noreferrer"> <span style="color:blue"> [Arxiv] </span></a>
<a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136850342.pdf" target="_blank" rel="noopener noreferrer"> <span style="color:blue"> [Paper] </span></a>
<a href="https://github.com/wjun0830/Localizable-Rotation" target="_blank" rel="noopener noreferrer"> <span style="color:blue"> [Code] </span></a>
<a href="https://www.youtube.com/watch?v=H4fX0KQfp2s" target="_blank" rel="noopener noreferrer"> <span style="color:blue"> [Video] </span></a> 
</td>
<td> <u>WonJun Moon</u>, Ji-Hwan Kim, Jae-Pil Heo </td>
</tr>

<tr>
<th scope="row"> ECCV 2022 </th>
<td> Difficulty-Aware Simulator for Open Set Recognition 
<br>
<a href="https://arxiv.org/abs/2207.10024" target="_blank" rel="noopener noreferrer"> <span style="color:blue"> [Arxiv] </span></a>
<a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136850360.pdf" target="_blank" rel="noopener noreferrer"> <span style="color:blue"> [Paper]</span> </a>
<a href="https://github.com/wjun0830/Difficulty-Aware-Simulator" target="_blank" rel="noopener noreferrer"> <span style="color:blue"> [Code] </span></a>
<a href="https://www.youtube.com/watch?v=0_q9wxDIYbE&t=197s" target="_blank" rel="noopener noreferrer"> <span style="color:blue"> [Video]</span> </a> 
</td>
<td> <u>WonJun Moon</u>, Junho Park, Hyun Seok Seong, Cheol-Ho Cho, Jae-Pil Heo </td>
</tr>


<tr>
<th scope="row"> CVPR 2022 (<b>oral</b>) </th>
<td> Task Discrepancy Maximization for Fine-grained Few-Shot Classification 
<br> 
<a href="https://arxiv.org/abs/2207.01376" target="_blank" rel="noopener noreferrer"> <span style="color:blue"> [Arxiv] </span></a>
<a href="https://openaccess.thecvf.com/content/CVPR2022/html/Lee_Task_Discrepancy_Maximization_for_Fine-Grained_Few-Shot_Classification_CVPR_2022_paper.html" target="_blank" rel="noopener noreferrer"> <span style="color:blue"> [Paper] </span></a>
<a href="https://github.com/leesb7426/CVPR2022-Task-Discrepancy-Maximization-for-Fine-grained-Few-Shot-Classification" target="_blank" rel="noopener noreferrer"> <span style="color:blue"> [Code]</span> </a>
</td>
<td> SuBeen Lee, <u>WonJun Moon</u>, Jae-Pil Heo </td>
</tr>
</table>


### Domestic

<table class="table table-sm table-borderless">

<tr>
<th scope="row"> KIISE 2022 </th>
<td> Learning from Data Imbalance with Class Grouping Loss
</td>
<td> <u>WonJun Moon</u>, Jae-Pil Heo </td>
</tr>

<tr>
<th scope="row"> KIISE 2020 </th>
<td> Mix-Contrastive Match
</td>
<td> <u>WonJun Moon</u>, Jae-Pil Heo </td>
</tr>
</table>

## Experience & Achievements
* Research Intern in Naver Cloud, hosted by <a href="https://scholar.google.co.kr/citations?user=42dUnrgAAAAJ&hl=ko" target="_blank" rel="noopener noreferrer"><span style="color:#6AADC2">Minho Shim</span></a> (Oct. 2023 - Mar. 2024)
* 2023 President's List, <a href="https://www.skku.edu/eng/index.do" target="_blank" rel="noopener noreferrer"><span style="color:#6AADC2">Sungkyunkwan University</span></a>
* 18th Scholarship student of the <a href="https://www.ikef.or.kr/" target="_blank" rel="noopener noreferrer"><span style="color:#6AADC2">Kwanjeong Educational Foundation</span></a>

## Reviewer
* Conference : CVPR, ICCV, ECCV, NeurIPS
* Journal : TPAMI 

## Education

* Ph.D. Dept of Artificial Intelligence, Sungkyunkwan University

* MS. Dept of Artificial Intelligence, Sungkyunkwan University
<br> GPA : 4.38 / 4.5

* Bs. Dept of Software, Sungkyunkwan University
<br> GPA : 4.42 / 4.5

* Language : Korean, English